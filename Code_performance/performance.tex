%\documentstyle[11pt,epsf]{article}
%\documentclass[preprint]{aastex}
\documentclass[10pt, preprint]{aastex}
\usepackage{epsfig}
\usepackage{epsf}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{pbox}

\pagestyle{plain}

\parskip=-1.0pt
\catcode`\@=11 % This allows us to modify PLAIN macros.
\def\Ch{{\it Chandra }}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}        
\def\lsim{\mathrel{\mathpalette\@versim<}}
\def\gsim{\mathrel{\mathpalette\@versim>}}
\def\la{\mathrel{\mathpalette\@versim<}}
\def\ga{\mathrel{\mathpalette\@versim>}}
\def\@versim#1#2{\vcenter{\offinterlineskip
        \ialign{$\m@th#1\hfil##\hfil$\crcr#2\crcr\sim\crcr } }}

\def\revtex@ver{4.0}	
\def\revtex@date{25 Apr 95}	
\def\revtex@org{AAS}		
\def\revtex@jnl{AAS}		
\def\revtex@genre{preprint}	
\typeout{\revtex@org\space \revtex@genre\space substyle,
v\revtex@ver\space <\revtex@date>.}
\def\revtex@pageid{\xdef\@thefnmark{\null}
\@footnotetext{This \revtex@genre\space was prepared with the
		   \revtex@org\space \LaTeX\ macros v\revtex@ver.}}
\def\references{{\center \Large REFERENCES}
%\def\references{\section{References}
%\def\references{\subsection*{REFERENCES}
\bgroup\parindent=\z@\parskip=\itemsep
\def\refpar{\par\hangindent=3em\hangafter=1}}
\def\endreferences{\refpar\egroup\revtex@pageid}
\def\thebibliography#1{\subsection*{REFERENCES}
\list{\null}{\leftmargin 3em\labelwidth\z@\labelsep\z@\itemindent -3em
\usecounter{enumi}}
\def\refpar{\relax}
\def\newblock{\hskip .11em plus .33em minus .07em}
\sloppy\clubpenalty4000\widowpenalty4000
\sfcode`\.=1000\relax}
\def\endthebibliography{\endlist\revtex@pageid}
\def\@biblabel#1{\relax}
\def\@cite#1#2{#1\if@tempswa , #2\fi}
\def\reference{\@ifnextchar\bgroup {\@reference}
	{\@latexerr{Missing key on reference command}
	{Each reference command should have a key corresponding to a markcite somewhere in the text}}}
\def\@reference#1{\relax\refpar}

\newcommand{\lya}{Ly$\alpha$\ }
\newcommand{\etal}{et al.\ }

\catcode`\@=12 % at signs are no longer letters

\def\apj{ApJ}                 % Astrophysical Journal
\def\apjl{ApJL}               % Astrophysical Journal, Letters
\def\apjs{ApJS}               % Astrophysical Journal, Supplement
\def\mnras{MNRAS}             % Monthly Notices of the RAS
\def\aap{A\&A}                % Astronomy and Astrophysics
\def\aaps{A\&AS}              % Astronomy and Astrophysics, Supplement 
\def\aj{AJ}                   % Astronomical Journal
\def\physrep{Phys.~Rep.}      % Physics Reports
\def\nat{Nature}              % Nature
\def\araa{ARA\&A}             % Annual Review of Astronomy and Astrophysics
\def\pasj{PASJ}               % Publ.Ast.Soc.Japan
\def\prd{Phys. Rev. D}        % Phys.Rev.D
\def\prl{Phys. Rev. Lett.}    % Phys.Rev.D

\textwidth 6.35 in
\evensidemargin 0.05in
\oddsidemargin 0.05in
%\leftmargin 1.05in
%\rightmargin 1.05in
\topmargin 0.2in
\textheight 8.8in 
\topmargin -.5in


\begin{document}
\input epsf

\title{Cosmology from Non-Linear Weak Lensing:\\Code Performance and Scaling}

\author{PI: Zolt\'an Haiman}

\affil{Department of Astronomy, Columbia University}

\section{Computational Methodology and Codes} \label{sec:methods}

As discussed in the Main Document, our simulation pipeline divides
into two parts: running cosmological N-body simulations, and then
performing ray-tracing calculations.  The project involves a large
number $N_{\rm sim}$ of independent N-body simulations, and creating
$N_{\rm map}$ independent realizations of $3.5\times3.5$ degree
lensing maps.  For the first part, we use the public N-body code
Gadget-2 \citep{VS05}.  This code is widely used for cosmological
structure formation simulations and is known to scale well.  For the
second part, we use our own implementation of a standard ray-tracing
algorithm outlined in \citet{H&M01}.  Here we give some more detail
about these codes, and the benchmarks we have obtained on the XSEDE
cluster Stampede.

\subsection{N-body Simulations}

\subsubsection{Initial Conditions}

In preparation for the N-body simulations, we first compute the matter
power spectrum with the Einstein-Boltzmann code CAMB \citep{CAMB}.
This code, written in Fortran, solves the Boltzmann equation
efficiently, with the Boltzmann hierarchies expressed in terms of line
of sight integrals \citep{CMBFAST}.    This code is fast, widely used,
and needs to be run only once per cosmological model, presenting a negligible
computatinal cost (Line~(1) in Table~1).

We next produce random realizations of the power spectrum -- in the
form of particle positions and velocities, which statistically
randomly sample the power spectrum.  This step is performed with a
proprietary version of the initial condition (IC) generator N-GenIC
kindly provided to us by Volker Springel (Heidelberg), the author of
Gadget-2.  This code creates cosmological initial conditions based on
the Zeldovich approximation, in a format directly compatible with
Gadget-2.  The code is MPI-parallel and suitable for creating very
large initial conditions.  This step has to be done once for each of
the $N_{\rm sim}$ independent N-body simulations. It again represents
a negligible computational expense, even for N-body boxes as large as
$1,024^3$ particles (Lines~(2) $\&$ (3) in Table~1).

\subsubsection{N-body simulations}

Gadget-2 is among the most widely used public codes in astrophysics
and cosmology.  It is capable of running cosmological N-body/SPH
structure formation simulations on massively parallel computers with
distributed memory, with an explicit communication model implemented
with the standardized MPI interface.  In this project, we use only the
N-body feature, which describes the formation of dark matter
structures.  Gadget computes gravitational forces with a hierarchical
tree algorithm.  We employ periodic boundary conditions.  The force
computation and the time stepping of Gadget are both fully adaptive,
speeding up our runs which reach well into the nonlinear regime.

We have benchmarked Gadget-2 on XSEDE Stampede, through Andrea Petri's
participation in an unrelated allocation.
%The results are simply stated: the Gadget runs consume nearly all of
%the requested computing time
As shown in Line~(4) in Table~1, our standard 512$^3$ particle runs
require 780 SUs, using 256 processors.  As already described in the
original code release paper \cite{VS05}, Gadget scales well for up to
1024$^3$ particles, but this test is outdated, and shows runs only up
to 124 processors.  We have verified on Stampede that the scaling
remains essentially perfect with 512$^3$ particles up to 512
processors.

For the N-body simulation sizes we run, either 256 or 512 processors
with 512MB of memory per core or more would work well. Because we want
to run many such simulations, we have modified the code in a way that
it can run multiple such independent simulations in parallel with just
one MPI job submission. Our version of the code has been in use for
many years on the IBM Blue Gene/L and /P at Brookhaven National
Laboratory.  Computing time scales completely linearly with the number
of simulations run -- this is unsurprising, because the multiple
simulations packaged in parallel into one run are completely
independent, and do not communicate with each other.  The I/O strain
from simulations is not very big, 250GB per simulation during the
course of a run. The GPFS file system on the Blue Gene/L and /P never
had any problem with it, i.e. this file system was able to handle the
write-out of many simulations in parallel, without impacting the
perfect scaling.  From our tests, we have found that the I/O on the
file system on Stampede is a factor of several faster.




\begin{table}[t!] \label{tab:benchmarks}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Step/Code} & \textbf{Description of Test} & \pbox{20cm}{\textbf{Runtime,}\\ \textbf{\# of cores}} & \textbf{Cost} & \textbf{Scaled Time} \\ \hline \hline \hline\hline
\pbox{20cm}{(1) Matter power \\ spectrum (CAMB)} & \pbox{20cm}{16 power spectra \\ in parallel} & \pbox{20cm}{1h 16m 51s\\16 cores} & 17.8 SU & 1.11 SU $\times N_{\mathrm{cosmo}}$ \\ \hline 
\pbox{20cm}{(2) Initial conditions\\ generation (N-GenIC)\\} & \pbox{20cm}{1 initial condition \\ ($512^3$ particles,\\ $240 h^{-1}$ Mpc)} & \pbox{20cm}{27s\\256 cores} & 1.92 SU & 1.92 SU $\times N_{\mathrm{sim,512}}$ \\ \hline
\pbox{20cm}{(3) Initial conditions\\ generation (N-GenIC)\\} & \pbox{20cm}{1 initial condition \\ ($1024^3$ particles,\\ $480 h^{-1}$ Mpc)} & \pbox{20cm}{47s\\512 cores} & 6.68 SU & 6.68 SU $\times N_{\mathrm{sim,1024}}$ \\ \hline
\pbox{20cm}{(4) $N$-body sim\\ (Gadget-2)} & \pbox{20cm}{1 simulation \\ ($512^3$ particles)} & \pbox{20cm}{3h 3m 16s\\256 cores} & 781.9 SU & 781.9 SU $\times N_{\mathrm{sim}}$ \\ \hline \hline \hline \hline
\pbox{20cm}{(5) Lens plane\\ generation} & \pbox{20cm}{9 planes per snapshot\\ (58 snapshots, 1 sim)} & \pbox{20cm}{7m 15s\\60 cores} & 7.7 SU & 0.12 SU$\times N_{\mathrm{snapshots}}$ \\ \hline
\pbox{20cm}{(6) Lens plane\\ generation} & \pbox{20cm}{9 planes per snapshot\\ (58 snapshots, 4 sim)} & \pbox{20cm}{6m 59s\\240 cores} & 27.9 SU & \pbox{20cm}{0.12 SU$\times N_{\mathrm{snapshots}}$ \\ $\times N_{\mathrm{sim}}$} \\ \hline \hline
\pbox{20cm}{(7) Ray-tracing \\ CFHT catalogs} & \pbox{20cm}{2 subfields $\times$ 32 realiz. \\ (from 1 simulation only)} & \pbox{20cm}{9m 36s\\64 cores} & 10.24 SU & \pbox{20cm}{0.16 SU $\times N_{\mathrm{subfields}}$\\ $\times N_{\mathrm{map}}\times N_{\mathrm{cosmo}}$} \\ \hline
\pbox{20cm}{(8) Ray-tracing \\ CFHT catalogs} & \pbox{20cm}{2 subfields $\times$ 256 realiz. \\ (mixing 5 simulations)} & \pbox{20cm}{10m 45s\\512 cores} & 91.73 SU & \pbox{20cm}{0.18 SU $\times N_{\mathrm{subfields}}$\\ $\times N_{\mathrm{map}}\times N_{\mathrm{cosmo}}$} \\ \hline \hline \hline \hline
\pbox{20cm}{(9) Analysis \\ 2D Maps} & \pbox{20cm}{Measuring Power spectrum, \\ peak counts, moments \\ and Minkowski functionals for 10\\ 2048$\times$2048 pixel maps} & \pbox{20cm}{2m 6s\\1 core} & 0.01 SU & \pbox{20cm}{0.035 SU $\times N_{\mathrm{maps}}$} \\ \hline
%\pbox{20cm}{9) WL Method 2\\ ($\kappa,\gamma$ maps)} & \pbox{20cm}{32 realizations $(\kappa,\gamma)$ \\ (1 simulation only)} & \pbox{20cm}{16m 50s\\64 cores} & 17.9 SU & \pbox{20cm}{0.55 SU $\times N_{\mathrm{map}}$\\ $\times N_{\mathrm{cosmo}}$} \\ \hline
\end{tabular}
\end{center}
\caption{\textit{Selection of benchmark tests we ran for our N-body
    simulations (upper half of the Table) and weak lensing ray tracing
    (bottom half of the Table) on XSEDE Stampede. The first column
    gives the type of test run and the code used; the second column
    gives a brief description; the third column gives the total
    runtime and number of cores used for the test; the forth column
    indicates the number of service units (SU) we were charged for the
    run, and the last column finally computes the number of SUs per
    simulation product computed from this run, which can then be
    scaled. The simulation products have been grouped into units
    (i.e. \# of independent N-body runs, \#of different redshift
    snapshots, \# of cosmological models, or the \# of $3.5\times3.5$
    subfields), which do not communicate with each other, so scaling
    between them is trivial.}}
\end{table}


\subsection{Weak Lensing Pipeline}

We have also benchmarked the performance of our weak lensing code
(``Inspector Gadget'') on XSEDE Stampede. As explained in the main
document, this code, written by collaborator Jan Kratochvil, consists
of two major components, the lens plane generation and the ray
tracing.

\subsubsection{Lens Planes}

The time for generation of lens planes is very small compared to the
time required for the underlying N-body simulations, as illustrated by
Lines (5) and (6) in Table~1. In test (5), we created nine different
lens planes (using random rotations and shifts) at each of the 58
redshift outpouts of a single N-body simulation. Test (6) repeats
this, except for simultaneously processing four N-body
simulations. This shows the linear scaling with the number of
simulations, demonstrating that we can process at least four N-body
simulations concurrently, without overwhelming I/O (scaling to more
simulations is irrelevant for this task because 4 simulations with all
their snapshots can be processed in cca. 10 minutes).  Note that this
step (making 9 lens planes) has to be performed once for each snapshot
of each simulation.

\subsubsection{Ray-tracing}

The next step in our pipeline is the actual ray-tracing, through the
sequence of up to 58 lens planes.  The CFHTLS catalog contains 4.2
million galaxies with well-measured shapes and accurate photometric
redshifts between redshift $0.2 < z < 1.3$.  We have benchmarked the
step of creating lensing maps from the lens planes, by performing
ray-tracing in the directions towards the $\approx$ 1 million galaxies
located in the first two of our 13 subfields covering the CFHT sky
area (see Figure 3 in the Main Document).  In line~(7) of Table~1, we
have created 32 realizations of the shear maps in these two subfields,
in our fiducial cosmology, using 64 processors on Stampede.  In
line~(8), we have increased the number of processors to 512, and
created 8 times as many (256) realizations.  To further randomize
these realizations, we have mixed the lens planes from five
independent N-body runs.  As this test shows, the scaling of the
ray-tracing step is also nearly perfect, with 8 times as many
realizations requiring a factor of 8.96 longer runtime; the small
difference is attributable to the overhead involved in the mixing of
planes from five different simulations.
\section{Storage Requirements}

Storing the results (particle positions at each output) of a single
Gadget simulation would require 250GB of storage space, which is not
feasible to store for all our simulations. We will therefore archive
only a small fraction of our full simulation data (100 simulations, or
25 TB), and will store only the lens planes instead.  While the lens
planes capture all of the information in the snapshot cubes required
for ray tracing, they use only about a tenth of the disk space. The
lens planes can be used at any later point in time to rapidly create
more maps for better statistics, for different galaxy positions, or
for different surveys. It is therefore crucial to preserve them. One
$4096 \times 4096$-pixel lens plane requires 64MB of disk space. There
are 9 lens planes per snapshot cube, and there are 60 snapshot cubes
in every N-body simulation. This results in 34.6GB of disk space for
all the lens planes from one N-body simulation. Since we have 2000
N-body simulations in our proposal, the total amount of disk space
required to store all of the lens planes is 69.2TB.

\bibliographystyle{jponew}
\bibliography{refs}

\end{document}
